---
title: "Intuitive explanations of key NLP Techniques using R code"
output:
  github_document:
    toc: true
    toc_depth: 2
always_allow_html: yes
---
```{r, echo=FALSE, include=TRUE, results="asis"}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
kable_table <- function(table, title) {
  kableExtra::kable(table, caption = title) %>%
    kable_styling(latex_options = "hold_position", full_width = F, bootstrap_options = c("striped", "condensed"), position = "left")
}
```

# Summary

1. We learn what the TF-IDF VSM and then LSA are intuitively. The TF-IDF VSM  can find which text documents are similar to each other in rank order. The highest ranking documents tend to have matching words that are **rarely** used across all documents.
2. TF-IDF VSM can sound intimidating because of the the technical language used. "Term" means word, "Frequency" means count. The "Vector Space Model" is the part of the calculation that finds the angle between lists of matching word weights. This angle measures the similarity of documents.
3. A very simple example is explained. It assumes no Maths or Natural Language Processing (NLP) knowledge. We use basic maths and simple cosine trigonometry.
4. TF-IDF VSM is calculated as follows:
    a. **Term Frequency (TF):** We first count how often each word is used within each document.
    b. **Inverse Document Frequency (IDF):** We count the number of documents that contain each word. Inverse means dividing the total number of documents by the number of documents each word occurs in. The division gives common words a low IDF value, and rare words a high IDF value.
    c. **TF-IDF:** For each word in each document, we multiply the word count in that document by the log of the TF multiplied by the IDF. Taking the log dampens the importance of the most rare words that are not as important as their raw untransformed value would indicate.
    d. **Words embedded as vectors:** We call the TF-IDF weights for each word in each document, vectors. When we line up the vectors for each document next to each other we call it a matrix. Representing words as weights in a matrix is called word embedding.
    e. **Vector Spae Model (VSM):** We measure the similarity of each document vector against every other document vector one-by-one by measuring the angle between them. This is called the cosine similarity. We measure vector similarity by angle instead of distance to compensate for documents of varying lengths. 
5. We can sometimes improve the detection of document similarity using Latent Semantic Analysis (LSA). "Latent" means hidden, "Semantic" is meaning (i.e. hidden meaning analysis). In LSA,  documents are given some of the information value from words **not** in the document at all. However, those words **are** found inside documents that are similar to them. This is done by manipulating the matrix of word counts using Singular Value Decomposition (SVD).
6. SVD is well worth learning intuitively too because it is a fundamental technique behind many key Data Science methods such as: data dimension reduction prior to Machine Learning, Principal Components Analysis (PCA), and solving linear equations.
7. TF-IDF VSM and SVD are called count based methods. Modern methods use the context of words such as Word2vec.
8. The simple TF-IDF VSM could be accurate enough for your problem. Sometimes LSA or word2vec give significant improvement to justify their use. Compare simple to more complex methods objectively before deciding which to use.


# What is this document for?

This document describes key NLP techniques like TF-IDF VSM, LSA and word2vecin plain English with simple working examples in R code you can run and adjust yourself. The aim is to help you build an **intuitive** deeper understanding of NLP. This will help you move on to understand more complex techniques, and realise that sometimes the simple technique works well enough. Deeper understanding will also help you identify where techniques are weak and strong. 

The [betterexplained](https://betterexplained.com/articles/adept-method/) website, the [Feynman Technique](https://medium.com/taking-note/learning-from-the-feynman-technique-5373014ad230), and David Robinson's explanations and simple R examples for [empirical Bayesian methods](http://varianceexplained.org/r/simulation-bayes-baseball/) are all inspirations for explaining important techniques intuitively. In this spirit of those teachers, this document does not assume any previous Maths or Natural Language Processing (NLP) knowledge.

So far, I have created two R code conversions below of good NLP technique worked examples. The first is of a TF-IDF VSM example from this [tutorial](http://www.minerazzi.com/tutorials/term-vector-3.pdf) (page 6). It is logically followed by an R code conversion of the example in an [Introduction to Latent Semantic Analysis](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf). The LSA example the tutorial uses is from the canonical [Indexing by Latent Semantic Analysis](http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf) paper from 1990.

# Where can I learn more about NLP?

The calculations used in the code are not intended for use in a real project. For a real text mining project use a popular NLP package in R or Python, for example: 

- [tidytext](https://github.com/juliasilge/tidytext)
- [quanteda](https://quanteda.io)
- [text2vec](http://text2vec.org/index.html)
- [scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).

Also, the following are excellent NLP tutorials taking you from basic to advanced knowledge, mostly assuming no prior knowledge:

- [Speech and Language processing](https://web.stanford.edu/~jurafsky/slp3/) book and associated [YouTube videos](https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm).
Particularly [Chapter 6 Vector Semantics](https://web.stanford.edu/~jurafsky/slp3/6.pdf) that covers all topics in this document very clearly.
- [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) 
- [NLP-Guidance](https://moj-analytical-services.github.io/NLP-guidance/) written by Sam Tazzyman in the MoJ.


# An example of TF-IDF VSM 

## Documents to be searched

In this example we use the search terms "gold silver truck" to find the document in Table 1 that matches most closely.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(magrittr)
library(kableExtra)

d1 <- c("Shipment of gold damaged in a fire.")
d2 <- c("Delivery of silver arrived in a silver truck.")
d3 <- c("Shipment of gold arrived in a truck.")
q1 <- c("gold silver truck")

query <- data.frame(q1) %>%
  tidyr::gather(key = "document", value = "text")

words <- data.frame(d1, d2, d3) %>%
  tidyr::gather(key = "document", value = "text")

kable_table(words, "Table 1: Three documents we want to search")
```

## Pre-processing the text

We first use the [Tidytext::unnest_tokens()](https://www.tidytextmining.com/tidytext.html#the-unnest_tokens-function) function to split the text for each document into one word per row. We then use  [dplyr count](https://suzan.rbind.io/2018/04/dplyr-tutorial-4/#counting-the-number-of-observations) to count how often each word appears in each document.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(tidytext)

seperate_words <- words %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::count(document, word, sort = TRUE) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(word, document)

kable_table(seperate_words, "Table 2: All words split out from each document")
```

## Create the Term-Document Matrix

Next we reshape Table 2 from a long [Tidy](https://www.tidytextmining.com/tidytext.html#contrasting-tidy-text-with-other-data-structures) table into a wide table. The wide table is called a [Term-Document Matrix](https://moj-analytical-services.github.io/NLP-guidance/Glossary.html#tdm) where the terms are shown as one word per row, and each document is represented by its own column.

The last column $d_{i}$ counts in how many **documents** each word occurs one or more times across all documents. This is called the Document Frequency.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
tdm <- seperate_words %>%
  tidyr::spread(key = document, value = n) %>%
  base::replace(is.na(.), 0) %>%
  dplyr::group_by_all() %>%
  dplyr::summarise(di = sum(d1 >= 1) + sum(d2 >= 1) + sum(d3 >= 1)) %>%
  dplyr::arrange(desc(di), word)

kable_table(tdm, "Table 3: Term Document Matrix (TDM)")
```

## Calculate IDF for each Word across all documents

The Inverse Document Frequency (IDF) value is a measure of the discriminatory power of each word globally across the collection of documents. The value is low for a word found in most documents, and high when it occurs in very few.

To calculate the IDF, for each word, divide the total number of documents $D$ (in this example it is 3) by the number of documents each word occurs in, $D/d_{i}$. The most frequent words that occur in every document will equal 1 (3/3 = 1) and the most rare words occurring in only one document have the highest value of 3 (3/1 = 3).  

Table 4 is sorted by $IDF_{i}$ in descending order. $IDF_{i}$ is the log of $D/d_{i}$. Rare words like "damaged" have the highest $IDF_{i}$ as they appear in only one document. While words in all of the documents do not offer any discrimination. They have an $IDF_{i}$ value of zero (e.g. "of" = log(3/3) = 0). Low $IDF_{i}$ value words are often removed in the data preparation stage of text mining. These very common words are known as "stop words".

```{r, echo=TRUE, warning=FALSE, message=FALSE}
tdm <- tdm %>%
  mutate(
    Ddi = 3 / di,
    IDFi = base::log10(3 / di)
  ) %>%
  mutate_at(6:7, funs(round(., 2))) %>%
  dplyr::arrange(desc(IDFi), word)

kable_table(tdm, "Table 4: TDM with Inverse Document Frequency")
```

Intuitively, the importance of a word in document will not increase proportionally with frequency. Therefore, a simple method to reduce the importance of rare words is to take the logarithm of the ratio of $D/d_{i}$. This is particularly important for rare words used in only one document. They will have an IDF value the same as the total number of documents which can be in the thousands or more. A further exploration of the theory of IDF and how it relates to Information theory is described in detail [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.7340&rep=rep1&type=pdf)

Typically, log of base-10 is used. Base-10 allows faster mental calculation. For example,log(1)=0, log(10)=1, log(100)=2, etc. It is also simple to transform $IDF_{i}$ back to the original ratio by raising 10 by that value. For example, the $IDF_{i}$ value for "damaged" can be returned to the raw $D/d_{i}$ ratio by raising 10 to the power of 0.4771213, ($10^{0.4771213}$ = 3).

## Calculate the weight for each word in each document 

We now search the collection of three documents with our search terms by adding the column called "query" to Table 5. For each word, we multiply its $IDF_{i}$ value by the Term Frequency, which is simply the number of times the word appears in both the query and in each document. This weights the word counts in each document and the query by the discriminatory information each word has been found to provide across all the documents globally.

Look again at Table 1. We can see "silver" is rare globally among all the documents. It is only used in document 2. And also, "silver" is a common word locally (as it is used twice in document 2). No other word is used twice in the same document. Look at the weights calculated for "silver" in Table 5. The importance of the word "silver" is reflected in the highest weight calculated in the whole matrix. It's because "silver" appears in $d_{2}$ two times, and when multiplied with its $IDF_{i}$ value gives it the weight 0.954 shown in the column $w_{i}2$.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
query_seperate_words <- query %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::count(document, word, sort = TRUE) %>%
  dplyr::ungroup()

joined <- tdm %>%
  dplyr::left_join(query_seperate_words, by = "word") %>%
  dplyr::select(-document) %>%
  base::replace(is.na(.), 0) %>%
  dplyr::rename(query = n) %>%
  dplyr::mutate(
    wiq = query * IDFi,
    wi1 = d1 * IDFi,
    wi2 = d2 * IDFi,
    wi3 = d3 * IDFi
  ) %>%
  dplyr::arrange(desc(IDFi), word) %>%
  mutate_at(6:11, funs(round(., 2)))

kable_table(joined, "Table 5: Weight word frequecies with the IDF value for each word")
```

## Compare query to all documents

To compensate for the effect of [document length](https://cmry.github.io/notes/euclidean-v-cosine), the standard way of quantifying the similarity between the words in the query and the words in each document is to measure the angle between the two vectors of words. This is called the [cosine of similarity](https://nlp.stanford.edu/IR-book/html/htmledition/dot-products-1.html).

To find the angle between the vectors we multiply each TF-IDF weighted word frequency in the query by the IDF weighted word frequency for each document, then sum all the values. This multiplication is also called the dot product. The dot product is the sum of the products of each component of the two vectors. 

These [examples](https://www.varsitytutors.com/precalculus-help/find-the-measure-of-an-angle-between-two-vectors), show that the dot product equals the product of the length of the two vectors and the [cosine of the angle](https://www.mathopenref.com/cosine.html), a.b = |a|.|b|.$cos\theta$

That is to say, the dot product of two vectors will be equal to the cosine of the angle between the vectors, times the lengths of each of the vectors.

In Table 7, the cosine similarity value shows that document 2 is the closest match to the search terms "`r q1`". We can understand why intuitively if we consider that document 2 contains two of the search terms ("silver" and "truck"). Also, document 2 is the only document to contain the globally rare word "silver". 

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vector_length <- function(vec) {
  return(sqrt(sum(vec^2, na.rm = TRUE)))
}

cosine_similarity <- function(a, b) {
  sum(crossprod(a, b)) /
    (vector_length(a) * vector_length(b))
}

d1cosine <- cosine_similarity(joined$wiq, joined$wi1)
d2cosine <- cosine_similarity(joined$wiq, joined$wi2)
d3cosine <- cosine_similarity(joined$wiq, joined$wi3)

cosine <- c(d1cosine, d2cosine, d3cosine)
document <- c("document1", "document2", "document3")
text <- c(d1, d2, d3)
query <- c(q1, q1, q1)

which_document <- data.frame(document, text, query, cosine) %>%
  dplyr::arrange(desc(cosine)) %>%
  mutate_at(4, funs(round(., 2)))

kable_table(which_document, "Table 7: Cosine similarity value between the query and each document") 
```

## Unit vectors to compare word similarity regardless of frequency

Another way to calculate the cosine of the angle is to [normalise](https://moj-analytical-services.github.io/NLP-guidance/Glossary.html#norm) each vector  into a "unit vector". 

The reason for normalising is well explained by [Dan Jurafsky](https://web.stanford.edu/~jurafsky/slp3/6.pdf) page (11), _"The dot product is higher if a vector is longer, with higher values in each dimension. More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency."_

The normalised columns (or unit vectors) can be seen below in Table 6 as qhat, d1hat, d2hat, d3hat. Normalising a vector means converting the vector to a length of 1 by dividing each value by the vector length. We calculate length by taking the square root of the sum of all squared values in each vector.

```{r, echo=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
unit_vector <- function(vec) {
  return(vec / (sqrt(sum(vec^2, na.rm = TRUE))))
}

joined$qhat <- unit_vector(joined$wiq)
joined$d1hat <- unit_vector(joined$wi1)
joined$d2hat <- unit_vector(joined$wi2)
joined$d3hat <- unit_vector(joined$wi3)

joined <- joined %>%
  mutate_at(6:15, funs(round(., 2)))

kable_table(joined, "Table 6: Converting weighted word frequenies into unit vectors ")
```
Once normalised, the dot product of two vectors computes the cosine of the angle between them. This simple dot product calculation of normalised vectors in Table 6 creates Table 7 below. We can see that documents 1 and 3 are most similar to each other with a value of 0.25 (Table 7).

```{r, echo=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
joined_matrix <- as.matrix(joined[, 13:16])
compare_all <- base::crossprod(joined_matrix, joined_matrix)
colnames(compare_all) <- c("q", "d1", "d2", "d3")
rownames(compare_all) <- c("q", "d1", "d2", "d3")

kable_table(round(compare_all, 2), "Table 7: Compare all vectors with cosine similarity")
```

# Beyond TF-IDF - Latent Semantic Analysis (LSA)

Representing documents as vectors is called embedding. We can sometimes improve how document words embedded in a matrix can find similar documents using Latent Semantic Analysis (LSA). "Latent" means hidden, "Semantic" is meaning (i.e. hidden meaning analysis). In LSA, a document is given some of the information value from words **not** inside the document, but those words are found inside documents that are similar to them. 

LSA uses Singular Value Decomposition (SVD). It is well worth learning SVD intuitively too as it a fundamental technique behind many key Data Science tools:

* Data dimension reduction prior to Machine Learning and Principal Components Analysis [PCA](https://stats.idre.ucla.edu/r/codefragments/svd_demos/) 
* [Image compression](https://towardsdatascience.com/singular-value-decomposition-with-example-in-r-948c3111aa43)
* [Solving](http://www.math.usu.edu/~corcoran/classes/old/07spring6550/examples/svd.pdf) linear equations

Below we convert to R code a clearly explained example from an [Introduction to Latent Semantic Analysis](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf). It uses the example from [Indexing by Latent Semantic Analysis](http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf).

The example takes the following nine titles, five about human computer interaction  (c1 to c5), and four about mathematical graph theory (m1 to m5). 

- c1: Human machine interface for ABC computer applications
- c2: A survey of user opinion of computer system response time
- c3: The EPS user interface management system
- c4: System and human system engineering testing of EPS
- c5: Relation of user perceived response time to error measurement
- m1: The generation of random, binary, ordered trees
- m2: The intersection graph of paths in trees
- m3: Graph minors IV: Widths of trees and well-quasi-ordering
- m4: Graph minors: A survey

First we calculate the Term Document Matrix (or count of words in each document).

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# https://tutorials.quanteda.io/basic-operations/tokens/tokens_select/
txt <- c(c1 = "Human machine interface for ABC computer applications",
c2 = "A survey of user opinion of computer system response time",
c3 = "The EPS user interface management system",
c4 = "System and human system engineering testing of EPS",
c5 = "Relation of user perceived response time to error measurement",
m1 = "The generation of random, binary, ordered trees",
m2 = "The intersection graph of paths in trees",
m3 = "Graph minors IV: Widths of trees and well-quasi-ordering",
m4 = "Graph minors: A survey")

toks <- quanteda::tokens(txt)
toks_nostop <-quanteda::tokens_select(toks, c("human","interface","computer","user","system","response","time","EPS","survey","trees","graph","minors")
, selection = "keep", padding = FALSE)

mydfm <- quanteda::dfm(toks_nostop)

# Convert to a term document matrix to match the example
tdm <- base::t(as.matrix(mydfm))

kable_table(tdm, "Table 1: The Term Document Matrix")
```

We "decompose" the above matrix in Table 1 into three other matrices using the R base function [svd()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/svd.html). This R function implements [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition). The transformation, *"can be [intuitively interpreted](https://en.wikipedia.org/wiki/Singular_value_decomposition#Intuitive_interpretations) as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection."* If we multiply together the three decomposed matrices this exactly re-create the original matrix.

Table 2 is the result of decomposing Table 1 using SVD into U (the orthogonal matrix), D (the diagonal matrix), and V' (the transposed orthogonal matrix). 
*(Note if you compare Table 2 to the example in [Indexing by Latent Semantic Analysis](http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf) page 406, some of the signs are different. The reason for this ambiguity is explained [here](https://prod-ng.sandia.gov/techlib-noauth/access-control.cgi/2007/076422.pdf).)*

```{r, echo=TRUE, warning=FALSE, message=FALSE}
s <- base::svd(tdm)

u <- round(s$u,2)
d <- round(base::diag(s$d, 9, 9),2) # placing values on the diagonal
v <- round(base::t(s$v),2) # transpose the matrix (swap rows with columns)

knitr::kable(list(u,d,v), caption = "Table 2: U orthogonal matrix, D diagonal matrix, V' transposed orthogoanl matrix")
```

To demonstrate that the three matrices U, D and V' above are a decomposition of the first, when multiplied together the result in Table 3 is the same as our original matrix in Table 1.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
reconstruct <- round(u %*% d %*% v,0)

colnames(reconstruct) <- colnames(tdm)
rownames(reconstruct) <- rownames(tdm)

kable_table(reconstruct, "Table 3: Reconstructing our orginal matrix")
```

We can now reduce the dimensionality if we select only the first two dimensions of each matrix as shown in Table 4 below.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# find largest singular values
s_red <- RSpectra::svds(tdm,2)

u_red <- round(s_red$u,2)
d_red <- round(base::diag(s_red$d, 2, 2),2)
v_red <- round(base::t(s_red$v),2)

knitr::kable(list(u_red,d_red,v_red), caption = "Table 4: U, D and V' after selecting first two dimesions")
```
When we multiply the three reduced matricies in Table 4 this creates Table 5 below. You can see that while the word "trees" is not in the title of m4 ("Graph minors: A survey"), "trees" does now have some weight (0.66, Table 5). This is because "trees" is in a document that is very similar (m3 = "Graph minors IV: Widths of trees and well-quasi-ordering"). Also, the original value of 1.00 for "survey" in Table 1, which appeared once in m4, has been replaced by 0.42.

Describing this intuitively, _"in constructing the reduced dimensional representation, SVD, with only values along two orthogonal dimensions to go on, has to estimate what words actually appear in each context by using only the information it has extracted. It does that by saying: This text segment is best described as having so much of abstract concept one and so much of abstract concept two, and this word has so much of concept one and so much of concept two, and combining those two pieces of information (by vector arithmetic), my best guess is that word X actually appeared 0.6 times in context Y."_ pages 12 & 14 of the [Introduction to Latent Semantic Analysis](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf).
_
```{r, echo=TRUE, warning=FALSE, message=FALSE}
final <- round(u_red %*% d_red %*% v_red,2)

colnames(final) <- colnames(tdm)
rownames(final) <- rownames(tdm)

kable_table(final, "Table 5: Multiplication of reduced matriices U, D and V'")
```

Reducing the dimensionality of a Term Document Matrix in this way with a "truncated" SVD of the first two columns of the three decomposed matricies may improve our ability to search documents when using cosine similarity (as we did before with the TF-IDF VSM). However, you should judge for yourself if this transformation gives better performance by comparing your search results with and without the trasnformation. The usefulness of will vary between corpora (collections of text documents).

# Word context - going beyond count based word embeddings - Word2vec 

TF-IDF VSM and LSA have been called [count based methods](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf). More recent methods use the context of words such as [word2vec](https://www.tensorflow.org/tutorials/representation/word2vec). Word embeddings like word2vec uses a context predicting approach. This method will also be explained intuitively here soon using this example from  [text2vec](http://text2vec.org/glove.html#word_embeddings).

# Naive Bayes and Sentiment Classification

A future addition in R code will include the simple count based naive bayes classifer on page 6 of [Chapter 4  Naive Bayes and Sentiment Classification](https://web.stanford.edu/~jurafsky/slp3/4.pdf) from the [Speech and Language processing](https://web.stanford.edu/~jurafsky/slp3/).